
@article{ab,
  title={A/B Testing: A Systematic Literature Review},
  author={Federico Quin and Danny Weyns and Matthias Galster and Camila Costa Silva},
  journal={ArXiv},
  year={2023},
  volume={abs/2308.04929},
  url={https://api.semanticscholar.org/CorpusID:260735919}
}

@book{e4e,
  author = {David Sweet},
  url = {https://www.manning.com/books/experimentation-for-engineers},
  isbn = {9781617298158},
  keywords = {A/B testing, Bayesian Ootimization},
  publisher = {Manning},
  title = {Experimentation for Engineers},
  subtitle = {From A/B testing to Bayesian optimization},
  year = 2023
}

@article{ms,
  title={Bayesian Optimization in Materials Science: A Survey},
  author={Lars Kotthoff and Hud Wahab and Patrick Johnson},
  journal={ArXiv},
  year={2021},
  volume={abs/2108.00002},
  url={https://api.semanticscholar.org/CorpusID:236772166}
}

@article{simopt,
  author       = {Satyajith Amaran and
                  Nikolaos V. Sahinidis and
                  Bikram Sharda and
                  Scott J. Bury},
  title        = {Simulation optimization: {A} review of algorithms and applications},
  journal      = {CoRR},
  volume       = {abs/1706.08591},
  year         = {2017},
  url          = {http://arxiv.org/abs/1706.08591},
  eprinttype    = {arXiv},
  eprint       = {1706.08591},
  timestamp    = {Mon, 13 Aug 2018 16:48:52 +0200},
  biburl       = {https://dblp.org/rec/journals/corr/AmaranSSB17.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@article{turbomoo,
  author       = {Samuel Daulton and
                  David Eriksson and
                  Maximilian Balandat and
                  Eytan Bakshy},
  title        = {Multi-Objective Bayesian Optimization over High-Dimensional Search
                  Spaces},
  journal      = {CoRR},
  volume       = {abs/2109.10964},
  year         = {2021},
  url          = {https://arxiv.org/abs/2109.10964},
  eprinttype    = {arXiv},
  eprint       = {2109.10964},
  timestamp    = {Mon, 27 Sep 2021 15:21:05 +0200},
  biburl       = {https://dblp.org/rec/journals/corr/abs-2109-10964.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@misc{ax,
  author = {Meta},
  title = {Ax},
  url = {https://ax.dev},
  year = {2023}
}

@misc{rembo,
      title={Bayesian Optimization in a Billion Dimensions via Random Embeddings}, 
      author={Ziyu Wang and Frank Hutter and Masrour Zoghi and David Matheson and Nando de Freitas},
      year={2016},
      eprint={1301.1942},
      archivePrefix={arXiv},
      primaryClass={stat.ML},
      url={https://arxiv.org/abs/1301.1942}, 
}

@article{turbo,
  author       = {David Eriksson and
                  Michael Pearce and
                  Jacob R. Gardner and
                  Ryan Turner and
                  Matthias Poloczek},
  title        = {Scalable Global Optimization via Local Bayesian Optimization},
  journal      = {CoRR},
  volume       = {abs/1910.01739},
  year         = {2019},
  url          = {http://arxiv.org/abs/1910.01739},
  eprinttype    = {arXiv},
  eprint       = {1910.01739},
  timestamp    = {Wed, 09 Oct 2019 14:07:58 +0200},
  biburl       = {https://dblp.org/rec/journals/corr/abs-1910-01739.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}


@misc{botorch_code,
  author = {Meta},
  title = {BoTorch},
  url = {https://botorch.org},
  year = {2024}
}

@InProceedings{cts,
  title = 	 {Cylindrical {T}hompson Sampling for High-Dimensional {B}ayesian Optimization},
  author =       {Rashidi, Bahador and Johnstonbaugh, Kerrick and Gao, Chao},
  booktitle = 	 {Proceedings of The 27th International Conference on Artificial Intelligence and Statistics},
  pages = 	 {3502--3510},
  year = 	 {2024},
  editor = 	 {Dasgupta, Sanjoy and Mandt, Stephan and Li, Yingzhen},
  volume = 	 {238},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {02--04 May},
  publisher =    {PMLR},
  pdf = 	 {https://proceedings.mlr.press/v238/rashidi24a/rashidi24a.pdf},
  url = 	 {https://proceedings.mlr.press/v238/rashidi24a.html},
  abstract = 	 {Many industrial and scientific applications require optimization of one or more objectives by tuning dozens or hundreds of input parameters. While Bayesian optimization has been a popular approach for the efficient optimization of blackbox functions, its performance decreases drastically as the dimensionality of the search space increases (i.e., above twenty). Recent advancements in high-dimensional Bayesian optimization (HDBO) seek to mitigate this issue through techniques such as adaptive local search with trust regions or dimensionality reduction using random embeddings. In this paper, we provide a close examination of these advancements and show that sampling strategy plays a prominent role and is key to tackling the curse-of-dimensionality. We then propose cylindrical Thompson sampling (CTS), a novel strategy that can be integrated into single- and multi-objective HDBO algorithms. We demonstrate this by integrating CTS as a modular component in state-of-the-art HDBO algorithms. We verify the effectiveness of CTS on both synthetic and real-world high-dimensional problems, and show that CTS largely enhances existing HDBO methods.}
}


@misc{bts,
      title={Asynchronous Parallel Bayesian Optimisation via Thompson Sampling}, 
      author={Kirthevasan Kandasamy and Akshay Krishnamurthy and Jeff Schneider and Barnabas Poczos},
      year={2017},
      eprint={1705.09236},
      archivePrefix={arXiv},
      primaryClass={stat.ML},
      url={https://arxiv.org/abs/1705.09236}, 
}


@book{gpbook,
  added-at = {2019-03-04T22:26:50.000+0100},
  author = {Rasmussen, Carl Edward and Williams, Christopher K. I.},
  biburl = {https://www.bibsonomy.org/bibtex/21bf82350cc051367b8c7828a86c9dc0a/rwhender},
  
  file = {:gaussian process book.pdf:PDF},
  interhash = {72c030472023000e0bdeeb06081c3764},
  intrahash = {1bf82350cc051367b8c7828a86c9dc0a},
  keywords = {imported},
  owner = {wesley},
  publisher = {The MIT Press},
  timestamp = {2019-03-04T22:29:38.000+0100},
  title = {Gaussian Processes for Machine Learning},
  year = 2006
}

@inproceedings{gpytorch,
author = {Gardner, Jacob R. and Pleiss, Geoff and Bindel, David and Weinberger, Kilian Q. and Wilson, Andrew Gordon},
title = {GPyTorch: blackbox matrix-matrix Gaussian process inference with GPU acceleration},
year = {2018},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {Despite advances in scalable models, the inference tools used for Gaussian processes (GPs) have yet to fully capitalize on developments in computing hardware. We present an efficient and general approach to GP inference based on Blackbox Matrix-Matrix multiplication (BBMM). BBMM inference uses a modified batched version of the conjugate gradients algorithm to derive all terms for training and inference in a single call. BBMM reduces the asymptotic complexity of exact GP inference from O(n3) to O(n2). Adapting this algorithm to scalable approximations and complex GP models simply requires a routine for efficient matrix-matrix multiplication with the kernel and its derivative. In addition, BBMM uses a specialized preconditioner to substantially speed up convergence. In experiments we show that BBMM effectively uses GPU hardware to dramatically accelerate both exact GP inference and scalable approximations. Additionally, we provide GPyTorch, a software platform for scalable GP inference via BBMM, built on PyTorch.},
booktitle = {Proceedings of the 32nd International Conference on Neural Information Processing Systems},
pages = {7587–7597},
numpages = {11},
location = {Montr\'{e}al, Canada},
series = {NIPS'18}
}


@article{love,
  author       = {Geoff Pleiss and
                  Jacob R. Gardner and
                  Kilian Q. Weinberger and
                  Andrew Gordon Wilson},
  title        = {Constant-Time Predictive Distributions for Gaussian Processes},
  journal      = {CoRR},
  volume       = {abs/1803.06058},
  year         = {2018},
  url          = {http://arxiv.org/abs/1803.06058},
  eprinttype    = {arXiv},
  eprint       = {1803.06058},
  timestamp    = {Mon, 13 Aug 2018 16:46:46 +0200},
  biburl       = {https://dblp.org/rec/journals/corr/abs-1803-06058.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@misc{optuna_code,
  author = {Optuna},
  title = {Optuna},
  url = {https://optuna.org},
  year = {2025}
}

@article{optuna,
  title={Optuna: A Next-generation Hyperparameter Optimization Framework},
  author={Takuya Akiba and Shotaro Sano and Toshihiko Yanase and Takeru Ohta and Masanori Koyama},
  journal={Proceedings of the 25th ACM SIGKDD International Conference on Knowledge Discovery \& Data Mining},
  year={2019},
  url={https://api.semanticscholar.org/CorpusID:196194314}
}

@misc{tpe,
      title={Tree-Structured Parzen Estimator: Understanding Its Algorithm Components and Their Roles for Better Empirical Performance}, 
      author={Shuhei Watanabe},
      year={2023},
      eprint={2304.11127},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2304.11127}, 
}

@misc{cmaes,
      title={The CMA Evolution Strategy: A Tutorial}, 
      author={Nikolaus Hansen},
      year={2023},
      eprint={1604.00772},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/1604.00772}, 
}

@InProceedings{forest,
author="Hutter, Frank
and Hoos, Holger H.
and Leyton-Brown, Kevin",
editor="Coello, Carlos A. Coello",
title="Sequential Model-Based Optimization for General Algorithm Configuration",
booktitle="Learning and Intelligent Optimization",
year="2011",
publisher="Springer Berlin Heidelberg",
address="Berlin, Heidelberg",
pages="507--523",
abstract="State-of-the-art algorithms for hard computational problems often expose many parameters that can be modified to improve empirical performance. However, manually exploring the resulting combinatorial space of parameter settings is tedious and tends to lead to unsatisfactory outcomes. Recently, automated approaches for solving this algorithm configuration problem have led to substantial improvements in the state of the art for solving various problems. One promising approach constructs explicit regression models to describe the dependence of target algorithm performance on parameter settings; however, this approach has so far been limited to the optimization of few numerical algorithm parameters on single instances. In this paper, we extend this paradigm for the first time to general algorithm configuration problems, allowing many categorical parameters and optimization for sets of instances. We experimentally validate our new algorithm configuration procedure by optimizing a local search and a tree search solver for the propositional satisfiability problem (SAT), as well as the commercial mixed integer programming (MIP) solver CPLEX. In these experiments, our procedure yielded state-of-the-art performance, and in many cases outperformed the previous best configuration approach.",
isbn="978-3-642-25566-3"
}

@inproceedings{dngo,
author = {Snoek, Jasper and Rippel, Oren and Swersky, Kevin and Kiros, Ryan and Satish, Nadathur and Sundaram, Narayanan and Patwary, Md. Mostofa Ali and Prabhat, Prabhat and Adams, Ryan P.},
title = {Scalable Bayesian optimization using deep neural networks},
year = {2015},
publisher = {JMLR.org},
abstract = {Bayesian optimization is an effective methodology for the global optimization of functions with expensive evaluations. It relies on querying a distribution over functions defined by a relatively cheap surrogate model. An accurate model for this distribution over functions is critical to the effectiveness of the approach, and is typically fit using Gaussian processes (GPs). However, since GPs scale cubically with the number of observations, it has been challenging to handle objectives whose optimization requires many evaluations, and as such, massively parallelizing the optimization.In this work, we explore the use of neural networks as an alternative to GPs to model distributions over functions. We show that performing adaptive basis function regression with a neural network as the parametric form performs competitively with state-of-the-art GP-based approaches, but scales linearly with the number of data rather than cubically. This allows us to achieve a previously intractable degree of parallelism, which we apply to large scale hyperparameter optimization, rapidly finding competitive models on benchmark object recognition tasks using convolutional networks, and image caption generation using neural language models.},
booktitle = {Proceedings of the 32nd International Conference on International Conference on Machine Learning - Volume 37},
pages = {2171–2180},
numpages = {10},
location = {Lille, France},
series = {ICML'15}
}

@article{cohen,
 ISSN = {0006341X, 15410420},
 URL = {http://www.jstor.org/stable/3001666},
 author = {William G. Cochran},
 journal = {Biometrics},
 number = {1},
 pages = {101--129},
 publisher = {[Wiley, International Biometric Society]},
 title = {The Combination of Estimates from Different Experiments},
 urldate = {2025-05-11},
 volume = {10},
 year = {1954}
}




@misc{faiss_code,
  author = {Meta},
  title = {Faiss},
  url = {https://ai.meta.com/tools/faiss/},
  year = {2024}
}

@article{greed,
author = {De Ath, George and Everson, Richard M. and Rahat, Alma A. M. and Fieldsend, Jonathan E.},
title = {Greed Is Good: Exploration and Exploitation Trade-offs in Bayesian Optimisation},
year = {2021},
issue_date = {June 2021},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {1},
number = {1},
url = {https://doi.org/10.1145/3425501},
doi = {10.1145/3425501},
abstract = {The performance of acquisition functions for Bayesian optimisation to locate the global optimum of continuous functions is investigated in terms of the Pareto front between exploration and exploitation. We show that Expected Improvement (EI) and the Upper Confidence Bound (UCB) always select solutions to be expensively evaluated on the Pareto front, but Probability of Improvement is not guaranteed to do so and Weighted Expected Improvement does so only for a restricted range of weights. We introduce two novel -greedy acquisition functions. Extensive empirical evaluation of these together with random search, purely exploratory, and purely exploitative search on 10 benchmark problems in 1 to 10 dimensions shows that -greedy algorithms are generally at least as effective as conventional acquisition functions (e.g., EI and UCB), particularly with a limited budget. In higher dimensions, -greedy approaches are shown to have improved performance over conventional approaches. These results are borne out on a real-world computational fluid dynamics optimisation problem and a robotics active learning problem. Our analysis and experiments suggest that the most effective strategy, particularly in higher dimensions, is to be mostly greedy, occasionally selecting a random exploratory solution.},
journal = {ACM Trans. Evol. Learn. Optim.},
month = apr,
articleno = {1},
numpages = {22},
keywords = {Bayesian optimisation, acquisition function, infill criteria, ε-greedy, exploration-exploitation trade-off}
}

@misc{sfu,
  author = {Surjanovic, Sonya and Bingham, Derek},
  title = {Virtual Library of Simulation Experiments: Optimization Test Problems},
  year = {2013},
  howpublished = {\url{http://www.sfu.ca/~ssurjano/optimization.html}},
  note = {Accessed: 2023-10-25}
}


@misc{gymnasium,
  author = {Farama},
  title = {Gymnasium},
  url = {https://gymnasium.farama.org/index.html},
  year = {2024}
}

@article{gym,
  author       = {Greg Brockman and
                  Vicki Cheung and
                  Ludwig Pettersson and
                  Jonas Schneider and
                  John Schulman and
                  Jie Tang and
                  Wojciech Zaremba},
  title        = {OpenAI Gym},
  journal      = {CoRR},
  volume       = {abs/1606.01540},
  year         = {2016},
  url          = {http://arxiv.org/abs/1606.01540},
  eprinttype    = {arXiv},
  eprint       = {1606.01540},
  timestamp    = {Fri, 08 Nov 2019 12:51:06 +0100},
  biburl       = {https://dblp.org/rec/journals/corr/BrockmanCPSSTZ16.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@misc{MOPTA08,
  author       = {Don R. Jones},
  title        = {MOPTA08 Automotive Benchmark Problem:
                  Large-Scale Multi‑Disciplinary Mass Optimization in the Auto Industry},
  howpublished = {Presentation at the \emph{Modeling and Optimization: Theory and Applications}
                  (MOPTA) Conference, 2008.  Slides and benchmark files available at
                  \url{https://www.miguelanjos.com/projects/3009-jones-benchmark}},
  year         = {2008},
  note         = {Accessed 11~May 2025}
}

@inproceedings{helicopter,
 author = {Kim, H. and Jordan, Michael and Sastry, Shankar and Ng, Andrew},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {S. Thrun and L. Saul and B. Sch\"{o}lkopf},
 pages = {},
 publisher = {MIT Press},
 title = {Autonomous Helicopter Flight via Reinforcement Learning},
 url = {https://proceedings.neurips.cc/paper_files/paper/2003/file/b427426b8acd2c2e53827970f2c2f526-Paper.pdf},
 volume = {16},
 year = {2003}
}

@inproceedings{ars,
 author = {Mania, Horia and Guy, Aurelia and Recht, Benjamin},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {S. Bengio and H. Wallach and H. Larochelle and K. Grauman and N. Cesa-Bianchi and R. Garnett},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {Simple random search of static linear policies is competitive for reinforcement learning},
 url = {https://proceedings.neurips.cc/paper_files/paper/2018/file/7634ea65a4e6d9041cfd3f7de18e334a-Paper.pdf},
 volume = {31},
 year = {2018}
}

@book{dace,
author = {Thomas J. Santner and Brian J. Williams and William I. Notz},
publisher = {Springer New York, NY},
isbn = {9781493988471},
title = {The Design and Analysis of Computer Experiments},
booktitle = {The Design and Analysis of Computer Experiments},
doi = {https://doi.org/10.1007/978-1-4939-8847-1},
url = {https://link.springer.com/book/10.1007/978-1-4939-8847-1},
year = {2019},
}

@misc{centerbias,
      title={The Evolutionary Computation Methods No One Should Use}, 
      author={Jakub Kudela},
      year={2023},
      eprint={2301.01984},
      archivePrefix={arXiv},
      primaryClass={cs.NE}
}