\documentclass{article}


% if you need to pass options to natbib, use, e.g.:
%     \PassOptionsToPackage{numbers, compress}{natbib}
% before loading neurips_2025


% ready for submission
% \usepackage{neurips_2025}


% to compile a preprint version, e.g., for submission to arXiv, add add the
% [preprint] option:
\usepackage[preprint]{neurips_2025}



% to compile a camera-ready version, add the [final] option, e.g.:
% \usepackage[final]{neurips_2025}



% to avoid loading the natbib package, add option nonatbib:
%    \usepackage[nonatbib]{neurips_2025}


\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\usepackage{xcolor}         % colors
\usepackage{amsmath}
\usepackage{graphicx} 
\usepackage{mathrsfs}
\usepackage{algorithm}
\usepackage{algpseudocodex}
\usepackage{subcaption}
\usepackage{tablefootnote}

\DeclareMathOperator*{\argmax}{argmax}
\DeclareMathOperator*{\argmin}{argmin}
\newcommand{\dd}{\mathop{}\!{d}}

\title{Taking the GP Out of the Loop}


% The \author macro works with any number of authors. There are two commands
% used to separate the names and addresses of multiple authors: \And and \AND.
%
% Using \And between authors leaves it to LaTeX to determine where to break the
% lines. Using \AND forces a line break at that point. So, if LaTeX puts 3 of 4
% authors names on the first line, and the last on the second line, try using
% \AND instead of \And before the third author name.


\author{
  David Sweet\\
  Department of Computer Science\\
  Yeshiva University\\
  New York, NY 10033 \\
  \texttt{david.sweet@yu.edu} 
\And
  Siddhant anand Jadhav\\
  Department of Data Science and Visualization\\
  Yeshiva University\\
  New York, NY 10033 \\
  \texttt{sjadhav1@yu.edu} \\
}


\begin{document}


\maketitle


\begin{abstract}

    Bayesian optimization (BO) has traditionally solved black box problems where evaluation is expensive and, therefore, design-evaluation pairs (i.e., observations) are few. Recently, there has been growing interest in applying BO to problems where evaluation is cheaper and, thus, observations are more plentiful. An impediment to scaling BO to many observations, $N$, is the $O(N^3)$ scaling of a na{\"i}ve query of the Gaussian process (GP) surrogate. Modern implementations reduce this to $O(N^2)$, but the GP remains a bottleneck. We propose Epistemic Nearest Neighbors (ENN), a surrogate that estimates function values and epistemic uncertainty from $K$ nearest-neighbor observations. ENN has $O(N)$ query time and omits hyperparameter fitting, leaving uncertainty uncalibrated. To accommodate the lack of calibration, we employ an acquisition method based on Pareto-optimal tradeoffs between predicted value and uncertainty. Our proposed method, TuRBO-ENN, replaces the GP surrogate in TuRBO with ENN and its Thompson sampling acquisition method with our Pareto-based alternative. We demonstrate numerically that TuRBO-ENN can reduce the time to generate proposals by one to two orders of magnitude compared to TuRBO and scales to thousands of observations.
  
\end{abstract}

\section{Introduction}

Bayesian optimization (BO) is commonly used in settings where evaluations are expensive, such as A/B testing (days to weeks) \cite{ab,e4e}, materials experiments (roughly 1 day) \cite{ms}. It has also been applied to simulation optimization problems in engineering, logistics, medicine, and other domains \cite{simopt}. More recently, BO has been used in settings where evaluations are fast and can be run in parallel—for example, large-scale simulations in engineering design. In such cases, thousands of evaluations may be generated during a single optimization process \cite{turbomoo}.

BO methods typically scale poorly with the number of observations, $N$, because proposals are generated by fitting and querying a Gaussian process (GP) surrogate. Modern, optimized implementations require $O(N^2)$ time per query. We refer to this setting as \emph{Bayesian optimization with many observations (BOMO)}, and present a method that reduces the proposal-time scaling to $O(N)$.

It is important to distinguish between BOMO and BO with many design parameters -- high-dimensional Bayesian optimization (HDBO).  Generally, we expect to need more observations to optimize more parameters since there are simply more possible designs to evaluate. This expectation is codified, for example, in Ax's (\cite{ax}) prescription to collect $2 \times D$ observations before fitting a surrogate (where $D$ is the number of design parameters, or \textit{dimensions}). However, the number of observations necessary to locate a good design depends on more than just $D$. For example, \cite{rembo} optimizes a one-billion-parameter function with only 500 observations, while \cite{turbomoo} takes 1500 observations to optimize a simulator with only 12 parameters. This work focuses on BOMO.

We propose a method consisting of two components: (i) a $K$-nearest neighbors surrogate, Epistemic Nearest Neighbors (ENN), which estimates function values and uncalibrated epistemic uncertainty, and (ii) an acquisition method designed to work without calibrated uncertainty. We integrate this approach into TuRBO \cite{turbo}, a popular BO method, by replacing its GP surrogate and Thompson sampling acquisition method with ENN and our acquisition method.



 \begin{figure}
      \centering
      \includegraphics[width=0.8\linewidth]{fig_scaling.pdf}
      \caption{Mean proposal time (in seconds) versus number of observations ($N$) for several Bayesian optimization methods.
        Subfigures show results for (a) $D = 300$, (b) $D = 1000$, and (c) a zoomed-in view of (b), averaged over many optimization runs (see Section~\ref{sec:numexp}).
        GP-based methods (\texttt{lei}, \texttt{ucb}, \texttt{turbo-1}) scale approximately as $O(N^2)$, while \texttt{optuna}, which uses a Parzen estimator, and our method (\texttt{turbo-enn-10}) scale linearly in $N$.
      }
      \label{fig:scaling}
    \end{figure}



\section{Background}
\label{sec:background}

	\subsection{Bayesian optimization}
        A Bayesian optimizer proposes a design, $x \in  =[0,1]^D$, given some observations, $\mathcal{D} = \{(x_m, y_m)\}_{m=1}^N$, where $y_m = f(x_m)$. A typical BO method consists of two components: a surrogate and an acquisition method. A surrogate is a model of $f(x)$ mapping a design, $x$, to both an estimate of $f(x)$, $\mu(x)$, and a measure of uncertainty in that estimate, $\sigma(x)$. An acquisition method determines the proposal, $x_p = \argmax_{x} \alpha(\mu(x), \sigma(x))$, where the $\argmax$ is found by numerical optimization (e.g., via BFGS \cite{botorch_code}) or by evaluating $\alpha(\cdot,\cdot)$ over a set of $x$ samples, for example, uniform in $[0,1]^D$ or following any number of sampling schemes \cite{bts,turbo,cts}.
        

    \subsubsection{Surrogate}
    The usual BO surrogate is a Gaussian process \cite{gpbook}. Given observations \(\mathcal{D}=\{(x_m,y_m)\}_{m=1}^N\), the GP posterior at a new point \(x\) has mean and variance

    \begin{equation}
    \begin{aligned}
    \mu(x) &= K(x_m,x)^{\!\top}  K(x_m,x_m)^{-1} y_m  \\
    \sigma^2(x) &= 1 - K(x_m, x)^{\!\top} K(x_m,x_m)^{-1} K(x_m,x).
    \end{aligned}
    \label{eq:GP}
    \end{equation}



    The $N \times N$ kernel matrix, $K(x_m,x_m)$, has as its elements $K(\cdot,\cdot)_{ij}=k(x_{m,i},x_{m,j})$, where $k(\cdot,\cdot)$ is a kernel function, often a squared exponential $k(x_{m,i},x_{m,j})=e^{-\lVert x_{m,i}-x_{m,j}\rVert^2/2\lambda}$, although others are common, too \cite{gpbook}. Similarly, the kernel vector, $K(x_m, x)_i=k(x_{m,i},x)$. The kernel matrix is the pairwise covariance between all observations, and the kernel vector is the covariance between the query point, $x$, and the observations.
    
    The $N \times N$ kernel matrix is a source of the GP's $O(N^2)$ scaling with number of observations as it takes $N(N-1)/2$ evaluations of $k(\cdot,\cdot)$ to construct it. A straightforward calculation of $K(x_m,x_m)^{-1}$ would worsen the scaling to $O(N^3)$, but an efficient conjugate-gradient algorithm reduces this, also, to $O(N^2)$ \cite{gpytorch}.

    The hyperparameter, $\lambda$, the kernel length-scale, is typically tuned to maximize the marginal log-likelihood of the observations, $\mathcal{D}$, by a numerical optimizer such as SGD  \cite{turbo} or BFGS \cite{botorch_code}.

    \subsubsection{Acquisition method}


There are many acquisition methods in the literature. Three common ones are:

\textbf{Upper Confidence Bound (UCB)}
    $x_p = \arg\max_x \bigl[ \mu(x) + \beta \sigma(x) \bigr]$, where $\beta$ is a constant. The first term encourages exploitation of $\mathcal{D}$, i.e. biasing $x_p$ towards a design that is expected to work well, while the second term encourages exploration of the design space so as to collect new observations that will improve future surrogates.

  \textbf{Expected Improvement (EI)}
    $x_p = \argmax_x  \mathbb{E}[\max\{0, y(x) - y_*\}]$, where $y_* = \max y_m$, and the expectation is taken over $y(x) \sim \mathcal{N}(\mu(x), \sigma^2(x))$.

  \textbf{Thompson Sampling (TS)]}
 $x_p = \argmax_x y(x)$, where $y(x)$ is a joint sample from the GP at a set of $x$ values. (A joint sample modifies equations~\eqref{eq:GP} to account for the covariance between each $x$ that is being sampled.)


All three methods rely on the GP’s uncertainty being calibrated. Calibration, i.e. hyperparameter tuning, requires multiple queries of the GP, each of which takes $O(N^2)$ time.

We next introduce a surrogate and companion acquisition method. The surrogate, ENN, reduces query time to $O(N)$, and the acquisition method does not require uncertainty calibration.

\section{Related work}
\label{sec:related}
    There are many approaches to scale BO to many observations.
    
    \textbf{Blackbox Matrix-Matrix multiplication} A conjugate-gradient algorithm replaces the inversion of $K(x_m,x_m)$ in equation \eqref{eq:GP} with a sequence of matrix multiplies, reducing the query time complexity of a GP from $O(N^3)$ to $O(N^2)$ \cite{gpytorch}. A Lanczos algorithm can speed up GP posterior sampling (e.g., used in Thompson sampling) to constant-in-$N$ \cite{love}.
    
    \textbf{Trust Region BO} The TuRBO algorithm \cite{turbo} reduces wall-clock time in two ways: (i) It occasionally restarts, discarding all previous observations, resetting $N$ to 0. (ii) It Thompson samples only within a trust region, a small subset of the overall design space where good designs are most likely, thus avoiding needless evaluations elsewhere.

    \textbf{Modeling $p_*(x)$} An open-source optimizer, Optuna \cite{optuna,optuna_code}, does not model $f(x)$. Instead, it models $p_*(x) = P\{x = \argmax_x f(x)\}$. The model is a Parzen estimator, a linear combination of functions of the observations, which has $O(N)$ query time. Optuna uses a modified EI-based acquisition method \cite{tpe}.

    Another optimizer, CMA-ES \cite{cmaes}, an evolution strategy (not a Bayesian optimizer), uses no surrogate at all. It models the distribution of the maximizer, $p_*(x)$, by evaluating $f(x)$ directly in batches of designs. After each batch is evaluated, all previous observations are discarded. Thus, the compute time of a CMA-ES proposal is $O(1)$, constant in $N$.

    Other methods of scaling to large $N$ replace the GP with a neural network \cite{dngo} or a random forest \cite{forest}. While fitting a neural network or random forest scales as $O(N)$, the fitting processes are complex and introduce many tunable hyperparameters. Query time depends on the model architecture and is independent of $N$.

    In contrast to the approaches above, our method scales linearly, $O(N)$, requires no hyperparameter fitting, and has a smaller constant factor, making it faster at every tested $N$. See Figure \ref{fig:scaling}.
    
\section{Epistemic Nearest Neighbors}
\label{sec:enn}






	\subsection{ENN surrogate}
    We define ENN by three properties:
    \begin{itemize}
        \item \textbf{Independence}: Each observation, $(x_m, y_m) \in \mathcal{D}$, produces an independent estimate of $f(x)$.
        \item \textbf{Mean}: $\mu(x \mid x_m,y_m) = y_m$.
        \item \textbf{Epistemic variance}: $\sigma^2(x \mid x_m,y_m) = d^2(x,x_m)$, where $d(x,x_m)$ denotes the (Euclidean) distance from $x$ to $x_m$.
    \end{itemize}

    Equating epistemic variance to squared distance from the measurement, $x_m$, captures the intuition that similar designs will have similar evaluations, $f(x)$.

    \paragraph{Combining estimates.}
    For a query point, $x$, we combine the estimates from its $K$ nearest neighbors into the linear combination with minimum variance, the precision-weighted average \cite{cohen}

    \[
    \mu(x) = \frac{ \sum_i^K \sigma^{-2}(x \mid x_i) \mu(x \mid x_i, y_i) }{ \sum_i^K \sigma^{-2}(x \mid x_i) },
      \qquad
    \sigma^2(x) = \operatorname{Var}[\mu(x)] = \frac{1}{\sum_i^K \sigma^{-2}(x \mid x_i)}
    \]

    Substituting from the properties above yields the ENN estimator:

    \begin{equation}
    \begin{aligned}
    \mu(x) = \frac{ \sum_i^K d^{-2}(x, x_i) y_i }{ \sum_i^K d^{-2}(x, x_i) }, \qquad
    \sigma^2(x) = \frac{1}{\sum_i^K d^{-2}(x,x_i)}
     \label{eq:ENN}
     \end{aligned}
    \end{equation}


\paragraph{Computational cost.}
Finding the $K$ nearest neighbors requires evaluating
$d(x,x_i)$ for all $N$ observations, costing
$O(KN)$ time per query. Treating the observations as independent relieves us from calculating the $O(N^2)$ pairwise covariances between observations as in the calculation of $K(x_m,x)$ in equations~\ref{eq:GP}.  Our implementation uses the Python module Faiss \cite{faiss_code} to find the $K$ nearest neighbors. Note that there is no hyperparameter-fitting step. (The impact of the choice of $K$ on performance in discussed in Appendix~\ref{sec:ablation}.)
    

     \begin{figure}
      \centering
      \includegraphics[width=0.8\linewidth]{enn_1d.pdf}
      \caption{Epistemic nearest neighbors (ENN) surrogate.  The dashed line shows $\mu(x)$ and the shaded region is proportional to 
             $\pm \sigma(x)$. Only the relative size of $\sigma(x)$ is meaningful because $\sigma(x)$ is uncalibrated. The solid red line is the function being estimated, $f(x)$.}
      \label{fig:surrogate}
    \end{figure}


    
    Figure~\ref{fig:surrogate} depicts the ENN surrogate for an inverted parabola with various numbers of observations, $N$, and settings of $K$. Both the mean and uncertainty become smoother as $K$ increases. Notice in the lower right subfigure ($K=9$, $N=10$) that the red line falls outside the gray area. A well-calibrated (i.e., fitted) model would contain the function in $\mu(x) - 2\sigma(x) < f(x) < \mu(x) + 2\sigma(x)$, with high probability. In the next section, Section~\ref{sec:acq}, we show that calibration is not
     necessary for effective BO.

    \subsection{Acquisition via Pareto Fronts}
    \label{sec:acq}

    Because the ENN surrogate is not calibrated, the scales of $\mu(x)$ and $\sigma(x)$ are
    unrelated, and acquisition methods that compare $\mu(x)$ to $\sigma(x)$ (e.g., UCB, EI, TS) would yield arbitrary proposals. We, therefore, treat acquisition as bi-objective optimization maximizing both $\mu(x)$ and $\sigma(x)$, an approach that is insensitive to the overall scales of $\mu(x)$ and $\sigma(x)$ (and was studied in\cite{greed}).
    

    \paragraph{Pareto dominance.}
    Let \(\mathcal{X}\subseteq[0,1]^D\) be the search space.
    For any two points \(x_i,x_j\in\mathcal{X}\),  
    \(x_i\) \emph{dominates} \(x_j\) if
    \(\mu(x_i)\ge\mu(x_j)\) and \(\sigma(x_i)\ge\sigma(x_j)\),
    with at least one inequality strict.
    The \textit{Pareto front} \(\mathcal{PF}(\mu,\sigma)\)
    is the set of non‑dominated points.
    In two objectives it is typically a one‑dimensional curve
    (Figure~\ref{fig:pareto}); each point on the front dominates every
    point lying below or to the left of it but none on the curve itself.

    \paragraph{Practical approximation.}
    We sample a finite candidate set $C = {x_i}$
     uniformly inside TuRBO’s trust
    region, compute $\mu(x)$ and $\sigma(x)$ for each $x_i$, and extract the non‑dominated subset $\mathcal{PF}_0 \subset C$. A proposal, or \textit{arm}, $x_a$ is drawn uniformly at random from $\mathcal{PF}_0$.
    If more than one arm is required, we keep sampling without replacement from $\mathcal{PF}_0$. If $\mathcal{PF}_0$ is exhausted, we find the next non-dominated subset,  $\mathcal{PF}_1 \subset C \setminus \mathcal{PF}_0 $. The process repeats until we have sampled the required number of arms. Code is available at [anonymized].

     \begin{figure}
      \centering
      \includegraphics[width=0.25\linewidth]{fig_pareto.pdf}
      \caption{$\mu(x)$ and $\sigma(x)$ for 100 candidate design points. The circled (red) points are the non-dominated subset.}
      \label{fig:pareto}
    \end{figure}
    
    
    \section{Numerical experiments}
    \label{sec:numexp}

    We benchmark TuRBO‑ENN on three categories of problem:  
    (i) 51 analytic test functions \cite{sfu},  
    (ii) Five reinforcement‑learning environments (LunarLander‑v3, Swimmer‑v5, Hopper‑v5, Ant‑v5, Humanoid‑v5,  \cite{gymnasium}), and  
    (iii) The MOPTA08 automotive simulator \cite{MOPTA08}.  
    Across all tasks, TuRBO‑ENN attains objective values comparable to TuRBO while reducing proposal time by an order of magnitude or more.  
    We compare to several baseline optimizers summarized in Table~\ref{tab:methods} and below.
    
    The computations in this section required an estimated 30,000 cpu-hours of compute time.


    \begin{table}[t]
    \caption{Optimization methods compared in Section~\ref{sec:numexp}.} 
    \label{tab:methods}
    \centering
    \small
    \begin{tabular}{lccccc}
    \toprule

     % (array([0.07921818, 0.25104041, 0.31869427, 0.35104713]),
 % array([0.01833146, 0.008077  , 0.0048544 , 0.01671245]))


    \textbf{Method} & \textbf{Surrogate} & \textbf{Acquisition} & \textbf{Cost / proposal}
     & \shortstack{\textbf{RL} \\ \textbf{Score}} & 
     \shortstack{\textbf{RL} \\ \textbf{Time}} \\
    \midrule
    \texttt{random}          & none         & Uniform sample            & $O(1)$ & $0.082 \pm .018$  & $0.00023$   \\[2pt]
    CMA--ES                  & none         &  Gaussian sample       & $O(1)$ & --- & ---\\[2pt]
    \texttt{optuna}          & Parzen KDE   & Modified EI             & $O(N)$ & --- & --   \\[2pt]
    \texttt{lei}             & GP           & Log‑EI                    & $O(N^{2})$ & ---  & ---\\[2pt]
    \texttt{ucb}             & GP           & UCB                       & $O(N^{2})$ & --- & ---\\[2pt]
    \texttt{turbo-1}         & GP           & \shortstack{Thompson sample\\trust region}        & $O(N^{2})$ & $0.35 \pm 0.017$ & $1.0$\\[2pt]
    \texttt{turbo-0}         & none         & \shortstack{Uniform sample\\trust region}    & $O(1)$ &  $0.25 \pm .004$  & $0.008$ \\[2pt]
    \texttt{turbo-enn-10}   & ENN ($K=10$) & \shortstack{Pareto($\mu$,$\sigma)$\\ front}              & $O(N)$  &  $0.32 \pm .005$   & $0.014$  \\
    \bottomrule
    \end{tabular}
    \end{table}


	\subsection{Scoring}
    \label{sec:scoring}
    To introduce our comparison methodology, we compare TuRBO-ENN to other optimizers on the 300-dimensional Ackley function, Figure~\ref{fig:ackley300d}. In each round of the optimization, one design, $x_n$, is proposed and evaluated, $y_n = f(x_n)$. The max-so-far, $y_{\max,n} = \max\{y_0,\dots,y(x_n)\}$, is plotted vs. $n$. We ran each optimization 10 times and depicted the mean of $y_{\max,n}$ by the dashed line and $\pm se$ (standard error) by the gray area. 

    \begin{figure}
      \centering
      \includegraphics[width=0.4\linewidth]{fig_ackley300d.pdf}
      \caption{Four different optimizers optimizing a 300-dimensional Ackley function. The legend shows the total time (in seconds) spent calculating proposals.}
      \label{fig:ackley300d}
    \end{figure}

    We can summarize each optimization method's performance in Figure~\ref{fig:ackley300d} with a single number, which we call the \textit{score}. At each round, $n$, find the maximum measured values so far for each method, $m$: $y_{\max,n,m}$. Rank these values across $m$ and scale: $r_{n,m} = [\texttt{rank}(y_{\max,n,m})-1]/(M-1)$, where $M$ is the number of methods. Repeat this for every round, $n$, then average over all $R$ rounds to get a score for each method: $s_m = \sum_n^R{r_{n,m}}/R$. The scores in figure~\ref{fig:ackley300d} are $s_{\texttt{turbo-1}}=1$, $s_{\texttt{turbo-enn-10}}=2/3$, $s_{\texttt{optuna}}=1/3$, and $s_{\texttt{random}}=0$.
    
    Using a normalized score enables us to average over runs on different functions which, in general, have different scales for $y$. Using a rank-based score prevents occasional, outlying result from dominating the average.

    
	\subsection{Pure functions}
    In these experiments we perform calculations similar to \ref{fig:ackley300d} for 51 test functions. To add variety to the function set and to avoid an artifact where an optimization method might coincidentally prefer to select points near a function's optimum (e.g., at the center of the parameter space \cite{centerbias}), we randomly distort each function. We move the center point to a uniformly-randomly chosen value, $x_0$. Along each axis, we distort like this
            \[x^\prime =
                \begin{cases}
                \frac{x - x_0}{1 + x_0} & x < x_0\\
                \frac{x - x_0}{1 - x_0} & x > x_0
                \end{cases}
            \]
            
    Note that the boundaries at $0$ and $1$ remain fixed. The value $x_0$ is set and fixed for the duration of an optimization run. We repeat the optimization 30 times for $D \le 100$ or 10 times for $D > 100$, each time with a different random distortion. We optimize each function for $\max(30, D)$ rounds.

    We calculate normalized scores for $y_{\max}$ as well as for $t_{proposal}$, the total time spent computing design proposals. Figure~\ref{fig:seq_high} compares TuRBO-ENN to TuRBO and other methods in various dimensions from 100-1000. Scores in each plot are averaged over 51 test functions. The full list of functions is available in the code repository [anonymized] and is taken from \cite{sfu}.
    
    The other optimization methods are summarized in Table~\ref{tab:methods}.  The methods \texttt{lei} and \texttt{ucb} use a GP surrogate, and \texttt{optuna} uses a tree Parzen model. \texttt{turbo-1} is the single-trust-region variant of TuRBO \cite{turbo}, which tracks a trust region (a box inside $[0,1]^D$) from round-to-round. \texttt{turbo-1} Thompson samples from $\max(5000, 2D)$ candidate $x$ values, $C$ in the trust region. \texttt{turbo-enn-10} replaces the Thompson samples with samples from the Pareto front (see Section~\ref{sec:acq}) of $C$ and uses ENN instead of a GP. We use $K=10$ for ENN estimation. (Other values are studied in Appendix~\ref{sec:ablation}.) For all optimizations, the first round of arms (proposed designs) is chosen by some method of random initialization. For all TuRBO-based methods the initialization method is a latin hypercube design \cite{dace}. EI and UCB are initialized with Sobol' samples \cite{dace}.

    We also include an ablation we call \texttt{turbo-0} in which we use no surrogate. Instead, we just select a design, $x$, uniformly randomly in the trust region. Comparing \texttt{turbo-0} to \texttt{random} and \texttt{turbo-1} allows one to disentangle the impact of the trust region logic from that of the surrogate. The column \emph{RL Score} shows average scores (as defined in Section~\ref{sec:scoring}) on the three RL problems in Figures~\ref{fig:tlunar}-~\ref{fig:hop}. Sampling in the trust region instead of the entire bounding box improves the score from $0.21$ (\texttt{random}) to $0.36$ (\texttt{turbo-0}). Incorporating the ENN surrogate increases it to $0.42$ (\texttt{turbo-enn-10}). Finally, switching from ENN to a GP further raises the score to $0.44$ (\texttt{turbo-1}).

    The column \emph{RL Time} lists the total time a method spent calculating proposals, normalized to the time spent by \texttt{turbo-1}. The normalization was performed once for each problem since the proposal time varies by problem.



    \begin{figure}
      \centering
      \includegraphics[width=0.9\linewidth]{fig_seq_high.pdf}
      \caption{Optimizations on 51 pure functions in dimensions 100-1000. The top row compares the maximal attained objective values. The bottom row compares wall-clock time. TuRBO-ENN is faster than GP methods yet finds objective values comparable to those found by TuRBO-1.}
      \label{fig:seq_high}
    \end{figure}

    Figure~\ref{fig:seq_high} summarizes our results. We see that vanilla BO methods, \texttt{lei} or \texttt{ucb}, perform best (top row) but also take the most time to generate a proposal (bottom row). Note that when $D=1000$, we exclude \texttt{lei} and \texttt{ucb} due to prohibitive computational demands. \footnote{Based on our runs of $D \in {30, 100, 300}$, we project that a single optimization of a function in $D=1000$ with \texttt{lei} would take nine hours.} \texttt{turbo-1} improves computation time but sacrifices some design quality. Finally, \texttt{turbo-enn-10} is significantly faster than \texttt{turbo-1} but still achieve comparable design quality, with quality increasing with $K$.
    
    The bottom row shows only the scores of each method's proposal time. For time in seconds, see figure~\ref{fig:scaling}. At the 1000th proposal, \texttt{turbo-enn-10} is over 30 times faster than \texttt{turbo-1}. Extending the curve for \texttt{lei} in the first plot to $N=1000$, we estimate that \texttt{turbo-enn-10} would make its 1000th proposal over 1800 times faster than \texttt{lei}.
    
	\subsection{Simulators}
    We next examine optimizations of several realistic problems. Most of the problems presented are from Gymnasium \cite{gymnasium}, the RL testbed originally known as OpenAI Gym \cite{gym}.



    \begin{figure}
      \centering
      \includegraphics[width=1\linewidth]{fig_tlunar.pdf}
      \caption{LunarLander-v3, $D=12$, using the controller presented in \cite{turbo}. \texttt{turbo-enn-1} performs comparably to \texttt{turbo-1}, which uses a GP, while achieving speeds nearly matching  \texttt{turbo-0}, which uses no surrogate at all.}
      
      \label{fig:tlunar}
    \end{figure}

    For every gymnasium environment we use “frozen noise”, i.e., each evaluation starts with a fixed random seed \cite{helicopter}.   Panel (a) in each figure runs for 100 rounds using 1 arm per round. Panel (b), for each evaluation, averages the simulation's return (i.e., the sum of the rewards over all steps of the simulation) over \texttt{num\_denoise = 10} different random seeds, proposing and evaluating 10 arms/round. Panel (c) averages over \texttt{num\_denoise = 50} random seeds, proposing 50 arms/round. Configuration (c) was chosen to match the right panel of Figure 3 in \cite{turbo}. All optimizations in this section were replicated 100 times, each time with a different set of random seeds, for variety. The legend shows the total time spent proposing arms over all rounds (averaged over replications), in seconds. The time required to run the \textit{simulations} is excluded. Figures~\ref{fig:swim} and \ref{fig:hop} follow the same pattern.

    
    \begin{figure}
      \centering
      \includegraphics[width=1.0\linewidth]{fig_swim.pdf}
      \caption{Swimmer-v5, $D=17$, using a linear controller, similar to \cite{ars}. The controller designed by \texttt{turbo-enn-10} performs as well as that designed by \texttt{turbo-1}, but the designs are proposed almost 50 times faster.}
      \label{fig:swim}
    \end{figure}

    \begin{figure}
      \centering
      \includegraphics[width=1.0\linewidth]{fig_hop.pdf}
      \caption{Hopper-v5, $D=34$, using a linear controller, similar to \cite{ars}. Performance of \texttt{turbo-1}, \texttt{turbo-enn-10}, and \texttt{turbo-0} is comparable (within the error areas).  }
      \label{fig:hop}
    \end{figure}

    In each case, we see \texttt{turbo-enn-10} producing designs of similar quality to those produced by \texttt{turbo-1}, but 10-100 times faster. Interesting, the method \texttt{turbo-0}, which has no surrogate at all -- just sampling uniformly from the trust region --  outperforms other optimization methods on several problems. Hopper-v5 does not benefit from a surrogate, at least to the within the error bars of our measurement. Nevertheless, including a surrogate (whether ENN or GP) generally further improves performance.

    Figure~\ref{fig:three} shows results for three more problems, MOPTA08, Ant-v5, and Humanoid-b5. We excluded \texttt{turbo-1} and \texttt{optuna} from the comparisons of Ant-v5 and Humanoid-v5 because a single optimization would take longer than our allotted 5-hour window. We include them here simply to demonstrate that it is possible to work with thousands of observations in hundreds or thousands of dimensions using our Bayesian optimization method.


    \begin{figure}
      \centering
      \includegraphics[width=1.0\linewidth]{fig_msh.pdf}
      \caption{(a) MOPTA08 \cite{MOPTA08} $D=124$ 10 arms/round (b) Ant-v5, $D=841$, 100 arms/round. (c) Humanoid-v5, $D=5861$. (b,c) use a linear controller, similar to \cite{ars}}
      \label{fig:three}
    \end{figure}

\section{Limitations and future work}
A shift from $O(N^2)$ to $O(N)$ allows BO to handle many more observations. An $O(1)$ algorithm, however, would make BO effective with arbitrarily large $N$. Such is the case for evolution strategies like CMA-ES. We also discuss, in Appendix~\ref{sec:ablation},  the potential to improve the dependence of ENN on $K$. 

Additionally, while TuRBO-ENN performs comparably to TuRBO (with a GP), it does not perform as well (at least on the pure functions) as LEI or UCB. TuRBO is a local, trust-region method that seeks global optima by restarting. LEI and UCB perform global search directly. It would be interesting to seek a global ENN algorithm that matches the performance of LEI or UCB but retains the $O(N)$ scaling and speed of TuRBO-ENN.

    
\section{Discussion}
\label{sec:discussion}

This paper asked whether we could speed up Bayesian optimization in the presence of many observations by removing the $O(N^2)$ bottleneck, the Gaussian process (GP) surrogate. Our numerical studies indicate that the answer is \emph{Yes}. We introduced a simple model, Epistemic Nearest Neighbors (ENN) and found that substituting it for the GP in TuRBO resulted in one to two orders of magnitude reduced proposal time in our tests and better scaling ($O(N)$) without significantly sacrificing the quality of the proposed designs.

An interesting finding of this study is that we do not need to calibrate the surrogate's uncertainty for it to be useful in acquisition. Pareto‑front sampling works with the uncalibrated uncertainty that ENN provides, eliminating hyperparameter optimization which simplifies the implementation and reduces running time.




%%%%%%%%







\begin{ack}

% https://neurips.cc/Conferences/2025/PaperInformation/FundingDisclosure

\end{ack}

\appendix


% may use \small font
% {
% \small

\bibliographystyle{plain}
\bibliography{main}  

% }

\appendix




    \section{Ablations}
    \label{sec:ablation}

    Our acquisition method (Section~\ref{sec:acq}) relies on both the ENN mean
    \(\mu(x)\) and its uncertainty \(\sigma(x)\).
    To test whether each component is necessary, we evaluate three
    ablations of \texttt{turbo‑enn‑10}:
    
    \begin{itemize}
      \item \textbf{\texttt{turbo‑enn‑mu‑10}}: propose the design with the
            largest \(\mu(x)\) and ignore \(\sigma(x)\).
      \item \textbf{\texttt{turbo‑enn‑sigma‑10}}: propose the design with
            the largest \(\sigma(x)\) and ignore \(\mu(x)\).
      \item \textbf{\texttt{turbo‑enn‑rand‑10}}: replace
            \(\sigma(x)\) by a uniform random value
            \(u\sim\mathcal{U}(0,1)\) when constructing the Pareto front,
            i.e.\ use \(\mathcal{PF}\bigl(\mu(x),u\bigr)\).
    \end{itemize}
    
   

    The first two ablations simply ignore one of $\mu(x)$ and $\sigma(x)$. The third tests whether our model for $\sigma(x)$ carries any useful information. It could be that random numbers drive exploration just as effectively, in which case we could simply omit our $\sigma(x)$ estimates.

     If both statistics contribute meaningfully, the full
    \texttt{turbo‑enn‑10} should outperform these ablations.

   \begin{figure}
      \centering
      \includegraphics[width=1.0\linewidth]{fig_pareto_ablation.pdf}
      \caption{It is important to consider both $\mu(x)$ and $\sigma(x)$ in the acquisition method. }
      \label{fig:pareto_ablation}
    \end{figure}

    TuRBO-ENN comes with a hyperparameter, $K$, which determines the number of neighbors used to form estimates. Figure~\ref{fig:sweep} explores the dependence of performance on $K$. The fact that there is a maximum in the function score vs. $K$ suggests that tuning $K$ at each round might be beneficial. Ideally, score would increase monotonically in $K$, which would enable one to subjectively trade off  evaluation speed (low $K$) for design quality (high $K$). Future work could explore modifications the ENN surrogate to achieve this goal.

   \begin{figure}
      \centering
      \includegraphics[width=1.0\linewidth]{fig_sweep.pdf}
      \caption{Score has a maximum in $K$.}
      \label{fig:sweep}
    \end{figure}

\end{document}